# -*- coding: utf-8 -*-
"""final_year_project.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1EHBEkaH4A_syr_QEhP4eUWEPfa-a8p0m
"""

print("hello world")

pip install -r requirements.txt

import os
import torch
from transformers import PegasusForConditionalGeneration, PegasusTokenizer
from keybert import KeyBERT
from sentence_splitter import SentenceSplitter
import nltk
from termcolor import colored

for resource in ["wordnet", "omw-1.4"]:
    try:
        nltk_path = nltk.find("corpora/{0}".format(resource))
    except Exception:
        nltk.download(resource)

from nltk.corpus import wordnet

class PegasusParaphraser:
    def __init__(self, num_beams=10):
        # self.module_dir = os.path.dirname(__file__)

        if(torch.cuda.is_available()):
            self.device = torch.device("cuda:0")
        else:
            self.device = torch.device("cpu:0")

        # Pegasus Tokenizer & Model for Paraphrasing
        print(colored("INFO", "green"),":\t  Loading Pegasus Tokenizer & Model for Paraphrasing.")
        paraphraser_model_name = "tuner007/pegasus_paraphrase"
        self.tokenizer = PegasusTokenizer.from_pretrained(paraphraser_model_name)
        self.model = PegasusForConditionalGeneration.from_pretrained(paraphraser_model_name).to(self.device)

        self.num_beams = num_beams

        self.splitter = SentenceSplitter(language='en')


    def paraphrase_text(self, text):
        sentence_list = self.splitter.split(text)
        batch = self.tokenizer(sentence_list,truncation=True, padding='longest', max_length=100, return_tensors="pt").to(self.device)
        translated = self.model.generate(**batch, max_length=60, num_beams=self.num_beams, num_return_sequences=1, temperature=1.5)
        tgt_text = self.tokenizer.batch_decode(translated, skip_special_tokens=True)

        paraphrased_text = " ".join(tgt_text)
        return paraphrased_text

from transformers.models.bert.modeling_bert import BertModel,BertForMaskedLM

class KeywordSynonyms:
    def __init__(self):
        # KeyBERT model for Keyword Extraction
        print(colored("INFO", "green"),":\t  Loading KeyBERT Model for Keyword Extraction.")
        self.keyword_extraction_model = KeyBERT()

    def extractKeywords(self, text):
        keywords = self.keyword_extraction_model.extract_keywords(text)
        # The output is of the format [('keyword1', 'score1'), ('keyword2', 'score2'), ...]
        return [x[0] for x in keywords]

    def getSynonyms(self, word, max_synonyms=6):
        synonyms = []
        for syn in wordnet.synsets(word):
            for l in syn.lemmas():
                synonyms.append(l.name().replace("_", " "))
                # Multi-word synonyms contain a '_' between the words, which needs to be replaced with a ' '

        return [x for x in list(set(synonyms)) if x.lower() != word.lower()][:max_synonyms]
        # Consider those synonyms that are not the same as the original word

    def getSynonymsForKeywords(self, text, max_synonyms=6):
        keywords = self.extractKeywords(text)
        kw_syn = {}
        for word in keywords:
            synonyms = self.getSynonyms(word, max_synonyms)
            if len(synonyms) > 0:
                kw_syn[word] = synonyms

        return kw_syn

paraphraser = PegasusParaphraser()

text = "This survey paper addresses the gap by pro- viding a structured overview of recent advancements in prompt engineering, categorized by application area. For each prompting approach, we provide a summary detailing the prompting methodology, its applications, the models involved, and the datasets utilized."

paraphrased_text = paraphraser.paraphrase_text(text)

paraphrased_text

text

kw_syn = KeywordSynonyms()

kw_syn.getSynonymsForKeywords(text)

GOOGLE_API_KEY = ``

!pip install google-generativeai langchain-google-genai streamlit

pip install transformers

from transformers import AutoTokenizer, AutoModelForSeq2SeqLM

# Load the tokenizer
tokenizer = AutoTokenizer.from_pretrained("facebook/mbart-large-cc25")

# Load the Gemini model
model = AutoModelForSeq2SeqLM.from_pretrained("facebook/mbart-large-cc25")

def paraphrase(text, num_outputs=1):
    # Tokenize input text
    inputs = tokenizer(text, return_tensors="pt", max_length=1024, truncation=True)

    # Generate paraphrased output
    outputs = model.generate(
        **inputs,
        max_length=1024,
        num_return_sequences=num_outputs,
        num_beams=5,
        early_stopping=True
    )

    # Decode the generated output tokens
    paraphrased_texts = [tokenizer.decode(output, skip_special_tokens=True) for output in outputs]

    return paraphrased_texts

# Example usage
input_text = "The quick brown fox jumps over the lazy dog."
paraphrased_outputs = paraphrase(input_text, num_outputs=3)
print("Original:", input_text)
print("Paraphrased:")
for i, output in enumerate(paraphrased_outputs):
    print(f"{i+1}: {output}")

input_text = "How do I improve my English?"
paraphrased_outputs = paraphrase(input_text, num_outputs=3)
print("Original:", input_text)
print("Paraphrased:")
for i, output in enumerate(paraphrased_outputs):
    print(f"{i+1}: {output}")

